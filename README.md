# Optimisation Dynamique de la Structure de Capital via Deep Reinforcement Learning

## ğŸ“‹ Vue d'ensemble

Ce projet implÃ©mente une approche novatrice utilisant le **Deep Reinforcement Learning (RL)** pour optimiser dynamiquement la structure de capital d'une entreprise. Contrairement aux modÃ¨les statiques traditionnels, notre agent RL apprend Ã  prendre des dÃ©cisions adaptatives (Ã©mission de dette, rachats d'actions, dividendes) en fonction des conditions de marchÃ© changeantes.

### CaractÃ©ristiques principales

- âœ… **3 Algorithmes RL**: PPO, SAC, TD3
- âœ… **Environnement personnalisÃ©** basÃ© sur Gymnasium
- âœ… **ModÃ¨le Ã©conomique complet** avec simulation rÃ©aliste
- âœ… **4 Politiques de benchmark** pour comparaison
- âœ… **5 ScÃ©narios Ã©conomiques** (baseline, rÃ©cession, boom, crise de crÃ©dit, haute volatilitÃ©)
- âœ… **MÃ©triques financiÃ¨res avancÃ©es** (WACC, couverture d'intÃ©rÃªts, rating de crÃ©dit)
- âœ… **Ã‰valuation statistique** robuste

## ğŸ—ï¸ Architecture du Projet

```
ProjetRL/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ environment/          # Environnement Gymnasium
â”‚   â”‚   â”œâ”€â”€ capital_structure_env.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/               # ModÃ¨les Ã©conomiques
â”‚   â”‚   â”œâ”€â”€ company.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents/               # Agents RL et benchmarks
â”‚   â”‚   â”œâ”€â”€ rl_agents.py
â”‚   â”‚   â”œâ”€â”€ baselines.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ utils/                # Utilitaires
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ finance.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ train.py                  # Script d'entraÃ®nement
â”œâ”€â”€ evaluate.py               # Script d'Ã©valuation
â”œâ”€â”€ config.yaml               # Configuration
â”œâ”€â”€ requirements.txt          # DÃ©pendances
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”œâ”€â”€ data/                     # DonnÃ©es
â”œâ”€â”€ logs/                     # Logs d'entraÃ®nement
â”œâ”€â”€ models/                   # ModÃ¨les sauvegardÃ©s
â””â”€â”€ results/                  # RÃ©sultats et graphiques
```

## ğŸš€ Installation

### PrÃ©requis
- Python 3.10+
- GPU NVIDIA (optionnel mais recommandÃ©)

### Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

## ğŸ“Š Configuration

Modifiez `config.yaml` pour ajuster:
- **ParamÃ¨tres de l'environnement** (cash flows initiaux, dette, equity)
- **HyperparamÃ¨tres RL** (learning rate, batch size, etc.)
- **ScÃ©narios Ã©conomiques** (volatilitÃ©, taux, spreads)
- **PondÃ©ration des rÃ©compenses** (valeur, flexibilitÃ©, dÃ©tresse)

## ğŸ¯ Utilisation

### EntraÃ®nement

```bash
# EntraÃ®ner PPO
python train.py --algorithm PPO --scenario baseline --timesteps 500000

# EntraÃ®ner tous les algorithmes
python train.py --algorithm all --scenario baseline

# EntraÃ®ner en rÃ©cession
python train.py --algorithm SAC --scenario recession --timesteps 500000
```

### Ã‰valuation et Comparaison

```bash
# Comparer tous les algorithmes
python evaluate.py --scenario baseline --episodes 10

# Sauvegarder les rÃ©sultats
python evaluate.py --scenario baseline --save-path results/baseline_eval

# Tester en rÃ©cession
python evaluate.py --scenario recession --episodes 20
```

## ğŸ“ Formulation MathÃ©matique

### MDP (Markov Decision Process)

**Ã‰tats**: sf={CF_t, D_t, E_t, C_t, Leverage_t, InterestCoverage_t, ...}

**Actions**: a_t = (Î”D_t, Î”E_t, Div_t) âˆˆ [-1, 1]Â³

**RÃ©compense**:
```
r_t = Î±Â·V_t + Î²Â·FlexScore_t - Î³Â·DistressCost_t - Î´Â·TransCost_t
```

OÃ¹:
- Î±=0.6: Poids de la valeur d'entreprise
- Î²=0.2: Poids de la flexibilitÃ© financiÃ¨re
- Î³=0.15: Poids des coÃ»ts de dÃ©tresse
- Î´=0.05: Poids des coÃ»ts de transaction

### Fonctions clÃ©s

**WACC** (CoÃ»t Moyen PondÃ©rÃ© du Capital):
```
WACC = (E/(D+E))Â·r_e + (D/(D+E))Â·r_dÂ·(1-T_c)
```

**Valeur d'Entreprise** (DCF simplifiÃ©):
```
V = Î£ CF_tÂ·(1+g)^t / (1+WACC)^t + Terminal Value
```

**CoÃ»ts de DÃ©tresse**:
```
DC(leverage) = 0.05Â·e^(3Â·(leverage-0.3)) si leverage > 0.3
```

## ğŸ“ˆ Algorithmes RL

### 1. **PPO (Proximal Policy Optimization)**
```
L^CLIP(Î¸) = E_t[min(r_t(Î¸)Â·Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Â·Ã‚_t)]
```
- **Avantages**: Stable, facile Ã  tuner
- **ParamÃ¨tres**: n_steps=2048, clip_range=0.2

### 2. **SAC (Soft Actor-Critic)**
```
J(Ï€) = E[Î£ r(s_t, a_t) + Î±Â·H(Ï€(Â·|s_t))]
```
- **Avantages**: Off-policy, exploration naturelle
- **ParamÃ¨tres**: tau=0.005, entropy coefficient=auto

### 3. **TD3 (Twin Delayed DDPG)**
- **Avantages**: Haute performance continue
- **ParamÃ¨tres**: policy_delay=2, noise=0.1

## ğŸ² Politiques de Benchmark

### 1. **Target Leverage**
Maintient un ratio d'endettement constant (D/E = 0.67)

### 2. **Pecking Order**
HiÃ©rarchie: Cash interne â†’ Dette â†’ Equity

### 3. **Market Timing**
Ã‰met quand marchÃ© favorable (faibles spreads, hauts P/B)

### 4. **Dynamic Trade-off**
Ã‰quilibre bÃ©nÃ©fices fiscaux et coÃ»ts de dÃ©tresse

## ğŸ“Š ScÃ©narios Ã‰conomiques

| ScÃ©nario | CF Growth | CF Vol | Rate Shock | Spread Shock |
|----------|-----------|--------|-----------|--------------|
| Baseline | 3% | 15% | 0% | 0% |
| Recession | -2% | 25% | +1% | +2% |
| Boom | 6% | 12% | -1% | -1% |
| Credit Crisis | 1% | 30% | +2% | +5% |
| High Vol | 3% | 30% | 0% | +1% |

## ğŸ“Š MÃ©triques d'Ã‰valuation

### Performance FinanciÃ¨re
- Valeur d'entreprise
- Rendement total des actionnaires (TSR)
- VolatilitÃ© de la valeur
- ProbabilitÃ© de faillite

### Efficience
- Distance du leverage optimal
- WACC moyen
- Vitesse d'ajustement
- Utilisation de la capacitÃ© de dette

### Robustesse
- Performance multi-rÃ©gimes
- SensibilitÃ© aux paramÃ¨tres
- StabilitÃ© de la politique

## ğŸ” RÃ©sultats Attendus

### HypothÃ¨ses
- **H1**: Agent RL surpasse les benchmarks statiques en valeur (+5%)
- **H2**: Meilleure adaptation aux chocs Ã©conomiques
- **H3**: Exploitation efficace du market timing
- **H4**: Maintien supÃ©rieur de la flexibilitÃ© financiÃ¨re

### Analyses PrÃ©vues
1. **Feature Importance** (SHAP values)
2. **Extraction de rÃ¨gles** interprÃ©tables
3. **Clustering** des Ã©tats Ã©conomiques
4. **Backtesting** sur entreprises S&P 500
5. **Analyse de sensibilitÃ©** robuste

## ğŸ› ï¸ Personnalisation

### Ajouter un nouvel algorithme

```python
from stable_baselines3 import A2C

class A2CAgent(RLAgent):
    def __init__(self, env, config, model_save_path="models"):
        super().__init__(env, config, "A2C", model_save_path)
        self.model = A2C('MlpPolicy', env, verbose=1)
```

### Ajouter un scÃ©nario Ã©conomique

```yaml
SCENARIOS:
  custom_scenario:
    cf_growth_mean: 0.04
    cf_volatility: 0.18
    rate_shock: 0.005
    spread_shock: 0.01
```

### Modifier la fonction de rÃ©compense

Ã‰ditez `_calculate_reward()` dans `capital_structure_env.py`

## ğŸ“ Fichiers Principaux

| Fichier | Description |
|---------|-------------|
| `src/environment/capital_structure_env.py` | Environnement Gymnasium principal |
| `src/models/company.py` | ModÃ¨le Ã©conomique d'entreprise |
| `src/agents/rl_agents.py` | ImplÃ©mentation des 3 algorithmes RL |
| `src/agents/baselines.py` | Politiques de benchmark |
| `src/utils/finance.py` | Fonctions financiÃ¨res (WACC, DCFF, etc.) |
| `train.py` | Script d'entraÃ®nement |
| `evaluate.py` | Script d'Ã©valuation et comparaison |

## ğŸ“š ThÃ©ories FinanciÃ¨res

Le projet s'inspire de:

1. **Modigliani-Miller (1958)**
   ```
   V_L = V_U + T_c Ã— D
   ```

2. **Trade-off Theory**
   ```
   V* = V_U + PV(Tax Shield) - PV(Financial Distress)
   ```

3. **Pecking Order Theory (Myers, 1984)**
   HiÃ©rarchie: Cash â†’ Debt â†’ Equity

## ğŸ”— RÃ©fÃ©rences

- Sutton & Barto (2018): *Reinforcement Learning: An Introduction*
- Schulman et al. (2017): *Proximal Policy Optimization Algorithms*
- Haarnoja et al. (2018): *Soft Actor-Critic: Off-policy Maximum Entropy Deep RL*
- Graham & Harvey (2001): *The Theory and Practice of Corporate Finance*

## ğŸ¤ Contribution

Pour contribuer:
1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/AmazingFeature`)
3. Commit (`git commit -m 'Add AmazingFeature'`)
4. Push (`git push origin feature/AmazingFeature`)
5. Open a Pull Request


